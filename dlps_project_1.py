# -*- coding: utf-8 -*-
"""dlps-project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pgjsBkkoiF6eIgwedH0HKusKNmHqe9U5
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_id = '11c_HhAyF2hqrOcdtTnwB0G90vqIsyuUz'
downloaded = drive.CreateFile({'id': file_id})

downloaded.GetContentFile('RUL_training-data.xlsx')

!pip install -q xlrd

import pandas as pd
df = pd.read_excel('RUL_training-data.xlsx')
print(df)

# we import required libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn
from sklearn.metrics import classification_report
from sklearn import metrics

x_input = df.to_numpy()
print("shape of input array is:")
print(x_input.shape)
print("----------------")

time = []
#time array is the collection of life of all engine times 

for i in range(0, x_input.shape[0]-1):
  if x_input[i][0] != x_input[i+1][0]:
    time.append(x_input[i][1])
  else:
    continue

time.append(x_input[x_input.shape[0]-1][1])
time = np.array(time)
time.shape 
#time.shape should give number of engines i.e. 133

#we will generate output array here

y_train = []

#array y gives RUL - Remaining Useful Life corresponding to each row in x

j = 0
for i in range(0, x_input.shape[0]):
  y_train.append(time[j] - x_input[i][1])
  if time[j] == x_input[i][1]:
    j = j+1

y_train = np.array(y_train)
y_train.shape
#shape of y (output) should be same as x (input)

x_train = np.delete(x_input, 0, 1)
x_train.shape
#deleting time column from input array x
# 'DO NOT RUN THIS CELL AGAIN'
# CAUTION CAUTION CAUTION CAUTION CAUTION CAUTION

#importing test data:

file_id = '1l_jOjgrQNVipZsRgSTHd0Qf81oloHmTX'
downloaded = drive.CreateFile({'id': file_id})

downloaded.GetContentFile('RUL_testing-data.xlsx')

df2 = pd.read_excel('RUL_testing-data.xlsx')
df2

x_input2 = df2.to_numpy()
x_input2.shape

time2 = []

for i in range(0, x_input2.shape[0]-1):
  if x_input2[i][0] != x_input2[i+1][0]:
    time2.append(x_input2[i][1])
  else:
    continue

time2.append(x_input2[x_input2.shape[0]-1][1])
time2 = np.array(time2)
time2.shape

y_test = []

#array y gives RUL - Remaining Useful Life corresponding to each row in x

j = 0
for i in range(0, x_input2.shape[0]):
  y_test.append(time2[j] - x_input2[i][1])
  if time2[j] == x_input2[i][1]:
    j = j+1

y_test = np.array(y_test)
y_test.shape

x_test = np.delete(x_input2, 0, 1)
x_test.shape
#deleting time column from input array x
# 'DO NOT RUN THIS CELL AGAIN'
# CAUTION CAUTION CAUTION CAUTION CAUTION CAUTION

# ab hamare paas x_train, y_train, x_test and y_test hai
# x_train and y_train se model train krke, x_test and y_test pr test krna hai...

# DATA VISUALIZATION

#1. plotting frequency of each column

fig, axes= plt.subplots(figsize=(30, 10), nrows=4, ncols=6)
#axes[0][1].set_title("Inverted Plot")

count = 1
for i in range(0, 4):
  for j in range(0, 6):
    if count <= 3:
      axes[i][j].set_title("op setting" + str(count))
    else:
      axes[i][j].set_title("sensor" + str(count-3))
    axes[i][j].hist(x_train[:, count], bins = 10)
    axes[i][j].set_ylabel('frequency')
    count = count+1

plt.tight_layout()
plt.show()

#2. Plotting pdf with random variable as different columns
fig, axes= plt.subplots(figsize=(30, 10), nrows=4, ncols=6)

count = 1
for i in range(0, 4):
  for j in range(0, 6):
    if count <= 3:
      axes[i][j].set_title("rv = op setting" + str(count))
    else:
      axes[i][j].set_title("rv = sensor" + str(count-3))
    frequency, bins = np.histogram(x_train[:,count],bins=8)
    pdf = frequency/(sum(frequency))
    axes[i][j].plot(bins[1:] , pdf)
    axes[i][j].set_ylabel('pdf')
    count = count+1

plt.tight_layout()
plt.show()

"""Till now, sensor 20, 21 ka pdf same hai;; 
sensor 13, 19 ka pdf same hai;; 
sensor 3, 9 ka lagbhag same

"""

#These graphs are particularly for engine number: 1
fig, axes= plt.subplots(figsize=(30, 10), nrows=4, ncols=6)

count = 1
for i in range(0, 4):
  for j in range(0, 6):
    if count <= 3:
      axes[i][j].set_title("op setting" + str(count))
    else:
      axes[i][j].set_title("sensor" + str(count-3))
    k = 0
    while y_train[k] != 0:
      axes[i][j].scatter(x_train[k, count], y_train[k])
      k = k+1
    axes[i][j].set_ylabel('RUL')
    count = count+1

plt.tight_layout()
plt.show()

#These graphs are particularly for engine number: 2
fig, axes= plt.subplots(figsize=(30, 10), nrows=4, ncols=6)

count = 1
for i in range(0, 4):
  for j in range(0, 6):
    if count <= 3:
      axes[i][j].set_title("op setting" + str(count))
    else:
      axes[i][j].set_title("sensor" + str(count-3))
    k = 223
    while y_train[k] != 0:
      axes[i][j].scatter(x_train[k, count], y_train[k])
      k = k+1
    axes[i][j].set_ylabel('RUL')
    count = count+1

plt.tight_layout()
plt.show()

# Variation in each feature versus other features - SCATTER PLOT
# option to operate the feature on y-axis
fig, axes= plt.subplots(figsize=(30, 10), nrows=4, ncols=6)

count = 1
for i in range(0, 4):
  for j in range(0, 6):
    if count <= 3:
      axes[i][j].set_title("op setting" + str(count))
    else:
      axes[i][j].set_title("sensor" + str(count-3))
    axes[i][j].scatter(x_train[:, count], x_train[:, 2], c = y_train)
    axes[i][j].set_ylabel('op setting 2')
    count = count+1

plt.tight_layout()
plt.show()

# heat map
plt.pcolor(df[2:])
plt.show()

import seaborn as sb

corr = df.corr() 
sb.heatmap(corr)

# we wnat to find maximum RUL
# We will kind of have 'int(max)' classes

max = -1
for i in range(0, y_train.shape[0]):
  if y_train[i] > max:
    max = y_train[i]

print(max)

from sklearn.decomposition import PCA

pca = PCA(n_components=3)
pca.fit(x_train[1:])
x_train_pca = pca.transform(x_train)
x_test_pca = pca.transform(x_test)

pca.explained_variance_ratio_.sum()

# only 3 features after PCA explain 99% of total variance

plt.scatter(x_train_pca[:, 0], x_train_pca[:, 1])
plt.xlabel("PCA component 1")
plt.ylabel("PCA component 2")
plt.show()

plt.scatter(x_train_pca[:, 1], x_train_pca[:, 2])
plt.xlabel("PCA component 2")
plt.ylabel("PCA component 3")
plt.show()

plt.scatter(x_train_pca[:, 0], x_train_pca[:, 2])
plt.xlabel("PCA component 1")
plt.ylabel("PCA component 3")
plt.show()

from sklearn.linear_model import LogisticRegression

reg = LogisticRegression().fit(x_train_pca, y_train)
y_pred = reg.predict(x_train_pca)

z = 0
for i in range(0, x_train_pca.shape[0]):
  if y_train[i] - y_pred[i] < 5:
    z = z+1

print(z)

z = 0
for i in range(0, x_train_pca.shape[0]):
  if y_train[i] - y_pred[i] < 5:
    z = z+1

print(z)

from sklearn.svm import SVC

model1 = SVC(kernel='linear', C=1)
ak1 = model1.fit(x_train_pca[0:5000, :], y_train[0:5000])
y_pred=ak1.predict(x_train_pca[0:5000, :])

z = 0
for i in range(0, 5000):
  if y_train[i] - y_pred[i] < 5:
    z = z+1

print(z)

#neural network from 2nd notebook

#neural network
import tensorflow as tf
from tensorflow.keras import layers, Model

input = layers.Input(shape=(3,),name='layer1')
k1 = layers.Dense(50,activation='tanh')(input)
k2 = layers.Dense(50,activation='tanh')(k1)
k3 = layers.Dense(50,activation='tanh')(k2)
output=layers.Dense(1)(k3)
model = Model(input,output)

model.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(lr = 0.01))

model.fit(x_train_pca, y_train, epochs = 50, verbose = 0)

y_pred = model.predict(x_test_pca)

z = 0
for i in range(0, y_test.shape[0]):
  if y_test[i] - y_pred[i] < -10:
    z = z+1

print(z)
print(y_test.shape[0])

